### 用Redis构建缓存集群

小规模的集群建议使用官方的 Redis Cluster，在节点数量不多的情况下，各方面表现都不错。

Redis Cluster 相比于单个节点的 Redis，能保存更多的数据，支持更多的并发，并且可以做到高可用，在单个节点故障的情况下，继续提供服务。

面对数据量大，进行分片；面对高可用，增加从节点，进行主从复制；面对高并发，读写分离

由于Redis Cluster是去中心化的，不适合再大一些规模的集群，原因是Redis每个节点，都保存了槽和节点的对应关系表，客户端可以访问任何一个Redis节点，当集群规模大的时候，这个关系表的更新就很慢了

再大一些规模的集群，可以考虑使用 twemproxy 或者 Codis 这类的基于代理的集群架构，放置在客户端和Redis集群中间，负责处理请求和响应，监控Redis集群节点，维护集群的元数据。虽然是开源方案，但是已经被很多公司在生产环境中验证过。

相比于代理方案，使用定制客户端的方案性能更好，很多大厂采用的都是类似的架构。其将代理服务的寻址功能放置在客户端，在客户端发起请求的时候，先去查询元数据，根据结果直接连接Redis节点



### MySQL to Redis同步

对于超大规模的系统，总是会有缓存穿透，而且往往小部分的缓存穿透就足以使MySQL数据库崩溃，为此，很多大厂选择将数据全部缓存到Redis中，所有的请求都到Redis中获取，因此需要保证更新MySQL数据后，必须及时去更新Redis。

使用分布式事务是不好的，会对数据更新服务有很强的侵入性。比如下单服务，由于分布式事务，会导致下单服务性能下降，如果Redis写入失败，还会导致下单失败，等于是降低了可用性和性能。

推荐的做法是启动一个更新Redis数据的服务，接收订单变更的MQ消息，然后更新Redis中的数据

更通用的做法是数据更新服务只负责更新数据库数据，而不用管更新缓存，有额外负责更新缓存的服务伪装成MySQL的从库，接收Binlog，解析Binlog，根据获取的数据变更信息去更新Redis缓存。

下面我们以比较常用的开源项目Canal为例，来演示一下如何实时接收 Binlog 更新 Redis 缓存。

```mysql
# 下载Canal
wget https://github.com/alibaba/canal/releases/download/canal-1.1.4/canal.deployer-1.1.4.tar.gz
tar zvfx canal.deployer-1.1.4.tar.gz
# 配置MySQL
[mysqld]
log-bin=mysql-bin # 开启Binlog
binlog-format=ROW # 设置Binlog格式为ROW
server_id=1 # 配置一个ServerID
# 在MySQL中给Canal开启一个专门的用户并授权
CREATE USER canal IDENTIFIED BY 'canal';  
GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%';
FLUSH PRIVILEGES;
# 重启MySQL，确保所有配置生效，查看Binlog文件和位置
> show master status

# 配置Canal，canal/conf/example/instance.properties
canal.instance.gtidon=false

# position info
canal.instance.master.address=127.0.0.1:3306
canal.instance.master.journal.name=binlog.000009
canal.instance.master.position=155
canal.instance.master.timestamp=
canal.instance.master.gtid=

# username/password
canal.instance.dbUsername=canal
canal.instance.dbPassword=canal
canal.instance.connectionCharset = UTF-8
canal.instance.defaultDatabaseName=test
# table regex
canal.instance.filter.regex=.*\\..

# 启动Canal
canal/bin/startup.sh

# 查看log，如果没有报错则成功
canal/logs/example/example.log
```

Canal 服务启动后，会开启一个端口（11111）等待客户端连接，客户端连接上 Canal 服务之后，可以从 Canal 服务拉取数据，每拉取一批数据，正确写入 Redis 之后，给 Canal 服务返回处理成功的响应。如果发生客户端程序宕机或者处理失败等异常情况，Canal 服务没收到处理成功的响应，下次客户端来拉取的还是同一批数据，这样就可以保证顺序并且不会丢数据。



### 对象存储

对象存储是原生的分布式存储系统，一般包括三部分：数据节点集群、元数据集群和网关集群（或者客户端）。数据节点集群负责保存对象数据，元数据集群负责保存集群的元数据，网关集群和客户端对外提供简单的访问 API，对内访问元数据和数据节点读写数据。

一个文件会被拆分为多个块，一块的大小是固定的（一般是几MB），块放置到容器中，容器是复制和迁移的基本单位，容器有多个副本，每个副本放置在不同的数据节点中



### 跨系统实时同步数据

一般的场景是需要将MySQL的数据同步到多个系统，方案是使用Canal接受MySQL的Binlog，然后发送到MQ，需要同步的多个系统订阅MQ的消息，更新自身的系统数据

下游的系统容易存在性能瓶颈，导致消息积压在MQ中，可以的做法是增加处理的进程数，同时要对应MQ主题的分区数量，对于具有因果一致性的数据需要放置到同一个分区中，保证消费的顺序性，Canal是支持根据KEY，把Binlog哈希到下游MQ不同分区中的



### 不停机安全更换数据库

常有的几个场景：

1. 对MySQL做分库分表，需要对原来单例数据库转移到数据库集群中；
2. 从传统的数据库集群转移到云上数据库；
3. 对数据进行额外的分析，数据需要迁移到其他系统，方便数据分析

注意：

1. 保证每个步骤都是可逆的，在出现问题的时候，能够快速回滚

操作：

1. 上线同步程序，旧库的数据迁移到新库，让新库的数据和旧库的保持数据同步；
2. 上线双写程序，上线对比补偿程序
3. 开启双写程序，关闭同步程序，读写仍然是旧库；
4. 开启对比补偿程序，保证新库和旧库的数据完全一致；
5. 逐步将读请求迁移到新库；
6. 下线对比补偿程序，关闭双写，此时读写都在新库
7. 下线旧库和双写程序

关于双写程序，这个双写的业务逻辑，一定是先写旧库，再写新库，并且以写旧库的结果为准。旧库写成功，新库写失败，返回写成功，但这个时候要记录日志，后续我们会用到这个日志来验证新库是否还有问题。旧库写失败，直接返回失败，就不写新库了。这么做的原因是，不能让新库影响到现有业务的可用性和数据准确性。上面这个过程如果出现问题，可以关闭双写，回滚到只读写旧库的状态。

关于对比补偿程序，是一个难点。对于像订单表这类不经常变化的数据，每次根据时间窗口对比数据，发现不一致的直接用旧库覆盖新库数据；对于像商品表这类随时变化的，如果有更新时间这一属性，那么发现数据不一致后，如果新库的更新时间小于旧库，那么就在下一个时间窗口在对比。另外时间窗口的结束时间，最好选择当前时间早1分钟。如果表格数据没有时间戳，那么只能通过Binlog获取数据变化，然后去新库对比和补偿



### 存储点击流数据

一般容易膨胀的数据有：点击流数据，监控数据，日志数据。而点击流数据指的是在APP或者网页上的埋点程序，记录用户的使用习惯，方便分析用户的行为，从而改善产品和运营。

可以使用Kafka或者Hadoop来存储点击流数据

Kafka不仅是消息队列，其本质上是分布式的流数据存储系统。其读写性能很高，能够有近乎“无限”的存储容量

Kafka对于数据是有分片的，比如设定了100个分片，但是每个分片总是会落到1个节点上，而1个节点的容量是有限的，因此每个分片总是会写满。那么增加分片呗？但是，Kafka增加分片后，不能将数据重新分配，无法将已有分片上的数据迁移到新的分片上，因此Kafka提供的其实是有限的存储容量。

而Hadoop是能够提供真正的无限存储容量，水平扩容即可。Hadoop+Hive可以提供一定的数据查询功能，而Kafka只能根据时间或者位点来提取数据。但是Hadoop吞吐量太小了。。。



### 面对海量数据，怎么查询更快

查询海量数据，一般是离线分析系统，如果数据量是在GB以下，还是可以用MySQL。如果是在TB以下，可以使用列式数据库，比如Cassandra、HBase，Elasticsearch。如果数据超过了TB了，那么需要考虑Hadoop生态圈的产品

但是仅仅根据数据级别来判断使用那些存储是远远不够的，需要根据查询来选择存储系统和数据结构，因为每个主流的存储系统都有其擅长的地方，当我们的查询需求刚好是存储系统擅长的，那么我们就使用他。

比如我们需要全文搜索的方式进行查询，那么Elasticsearch就是最好的选择了。

比如对于物流数据，可以被很多其他系统分析，针对每个下游系统都需要根据查询需求，去选择特定的方式，而不是说用一个数据库，一个数据结构去解决所有问题，对于仓库补货，分析物流数据来对仓库进行提前补货，其区域性很强，根据区域进行分片会很好提高查询。对于物流规划，查询方式是多变的，可以把数据放到Hive中，按照时间进行分片



### New SQL

New SQL就是兼顾了Old SQL和No SQL的优点：

- 完整地支持 SQL 和 ACID，提供和 Old SQL 隔离级别相当的事务能力；

- 高性能、高可靠、高可用，支持水平扩容。

CockRoach是一款New SQL。它的架构是一个分层架构，最上层是SQL层，下一层是Structured Data API，再下一层是Distributed, Monolithic KV Store，最下面是数据存储层。使用分布式KV存储存储数据，根据范围进行数据分片。使用Raft协议实现每个分片的高可用、高可靠和一致性。元数据存储在每个节点上，通过流言协议进行传播。用了 RocksDB 作为它的 KV 单机存储引擎。

CockroachDB 提供了另外两种隔离级别，分别是：Snapshot Isolation (SI) 和 Serializable Snapshot Isolation (SSI)，其中 SSI 是 CockroachDB 默认的隔离级别。两者都能够避免脏读、幻读、不可重复度，但是SI会导致写倾斜，写倾斜表达的是因为没有检测读写冲突，也没有加锁，导致数据写错了。CockroachDB已经能够提供和ACID差不多一样的功能了



### RocksDB

不丢失数据的高性能KV存储系统，读写性能甚至和Redis在同一个级别，而RocksDB是能够保证数据不丢失的，Redis会丢失数据

内存+磁盘，顺序写入，使用数据结构LSM-Tree来保证顺序写入数据的同时还能够保证良好的查询性能，LSM-Tree是一个复合的数据结构，它包含了 WAL（Write Ahead Log）、跳表（SkipList）和一个分层的有序表（SSTable，Sorted String Table）。

当RocksDB接受到一条写请求，会将请求先写入磁盘的WAL日志中，这样就保证了数据的可靠性，这儿性能很好。然后数据会写入到内存的MenTable中，这个是根据KEY组织的跳表，写入MemTable后就可以返回成功了，MemTable的大小是32M，满了之后会转换为Immutable MemTable，在创建一个新的MemTable。后台进程则不断地把Immutable MemTable写入到磁盘，写入的文件也是按照KEY排序的SSTable，这儿也是顺序写。

虽然每个SSTable内的KEY是有序的，但是SSTable之间的KEY是无序的，因此将SSTable分很多层，下一层的容量是上一层的10倍，当某一层写满后，就会触发将数据合并到下一层，合并后本层的SSTable就可以删除了，这样每一层的SSTable是有序的，SSTable内的KEY也是有序的，这样就便于查找了。

LSM-Tree 的数据结构可以看出来，这种数据结构还是偏向于写入性能的优化，更适合在线交易类场景，因为在这类场景下，需要频繁写入数据。



### 结束语

没有足够的技术积累，比较难以深入理解书本上的技术知识和原理具体的应用，唯有体验过问题，遇到过困惑，再去看书本的内容，才能够深刻的理解

对于技术积累，只有多写代码，多做项目